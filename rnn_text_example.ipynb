{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_text_example",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUgSMijpDsRj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespear.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvkwse9zESQ9",
        "outputId": "29b81637-fe80-4f77-bcb2-76502a91cfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, 'r').read()\n",
        "print(f'length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PkiSap1E0-p",
        "outputId": "56b1b3c9-cbaf-447a-b4c7-361ce75f7b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKoGgQN6J2iI",
        "outputId": "3461820b-5f65-4356-b21f-88087072d848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ9RkvrSIg5r",
        "outputId": "b47fe008-f7d0-4df2-ba70-c5f09fdadcab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Before training, you need to convert the strings to a numerical representation.\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding = 'UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b61WVuVFLU76",
        "outputId": "e4f6b38f-4a3c-4950-9bb9-d7fcabab48e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary = list(vocab), mask_token = None)\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmqi1xBbMZpi",
        "outputId": "8cf8d9d4-0d63-4dfa-d1eb-2f58716b68bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arYbmM67MzmD",
        "outputId": "275a6efe-f103-419f-8b1f-7c34347448e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars,axis = -1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI0JFqyeXA5M",
        "outputId": "bc070e46-cb94-45b9-a88a-2283a55ef6fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis = -1)"
      ],
      "metadata": {
        "id": "7rYRbOBXZJJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text,'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "ids_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFX55N3cT7EJ",
        "outputId": "6877c81b-6b32-4ed4-e240-d886ef9960bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int64>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "  print(chars_from_ids(ids).numpy().decode('utf-8'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVRiwA_oTZTD",
        "outputId": "88ca6927-8a58-4cb9-f273-ba046c001359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder = True)\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YFQD2baaJBD",
        "outputId": "c0b738b8-7e1d-40ac-a744-0836471ef808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n"
      ],
      "metadata": {
        "id": "ETrGCTucmiD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list('tensorflow'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEAeWsKzn-Og",
        "outputId": "c1dc44fc-4e3f-4b04-e21c-d5cabb58a9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['t', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9McbdhooIfq",
        "outputId": "10ff3b7e-3acf-450b-9ad7-22dce87c703b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7AMxfQXotUd",
        "outputId": "763f5e26-9452-4d1b-84bb-32ac9874b3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences, \n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
        "# it maintains a buffer in which it shuffles elements)\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkOIG6-V7m53",
        "outputId": "cc2be962-b726-4f97-cb85-e2eaf83ea99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "#The embedding dimension\n",
        "embedding_dim = 256\n",
        "#Number of RNN Units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "_xgxHrtS-gsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "model = MyModel (\n",
        "        #be sure the vocabulary size matches the 'StringLookup' layers.\n",
        "        vocab_size = len(ids_from_chars.get_vocabulary()), embedding_dim = embedding_dim, rnn_units = rnn_units\n",
        ")\n"
      ],
      "metadata": {
        "id": "reNmEh-I_Hc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFwQjN9oF6d3",
        "outputId": "92802e50-a05f-418c-a3c7-10e1ab5e5fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-fyJvbXHMVb",
        "outputId": "8a304830-63cc-4310-b604-a94c8c9d2bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKwwSTtRHQWs",
        "outputId": "02aa082d-9807-471a-96e0-e5733998eb77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([39,  9, 13, 45,  5, 23,  8, 63, 31, 12, 52, 22,  9, 53,  3, 46, 38,\n",
              "       39, 38, 37,  8,  0, 32, 37, 65, 51,  2, 50, 46, 35, 26, 50, 57, 49,\n",
              "       22,  3, 15, 64, 54, 58,  2,  5, 14, 52, 41, 20, 30, 44, 64,  5, 56,\n",
              "       57, 16, 42,  1,  4, 31, 43, 46, 32, 48, 29, 13, 57, 18,  4, 65, 52,\n",
              "       13, 63, 16, 63, 29, 29, 19, 52, 62, 52,  3, 24,  4, 33,  8, 43, 30,\n",
              "       21, 55, 21,  1, 25, 60, 17, 61, 54, 25, 11, 26, 55,  6, 41])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZEcZ-0GHdkA",
        "outputId": "18e8c2e4-02d8-43cc-d6f1-6be1627906a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\":\\nThen was I going prisoner to the Tower,\\nBy the suggestion of the queen's allies;\\nBut now, I tell t\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"Z.?f&J-xR;mI.n!gYZYX-[UNK]SXzl kgVMkrjI!Byos &AmbGQey&qrCc\\n$RdgSiP?rE$zm?xCxPPFmwm!K$T-dQHpH\\nLuDvoL:Mp'b\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "nsWYLnFqJR3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqSWNQkeKIHo",
        "outputId": "5ac7b920-8520-41fd-acac-ba9c32f65e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.190397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSK7_Cj_MLxU",
        "outputId": "98ebd210-6328-4bf0-80e6-7a2a7ce7c2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.048996"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "5BkGx7MhMNG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "BeYulcWQMQoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30"
      ],
      "metadata": {
        "id": "ljnVG5UyMZx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ff7ny0aMk1n",
        "outputId": "d63dbac3-a931-47e4-d87c-7550ff8a3afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 15s 52ms/step - loss: 2.7266\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.9997\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.7258\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.5662\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.4650\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.3953\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.3419\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.2972\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.2563\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.2173\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.1791\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.1398\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.0985\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.0533\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.0064\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.9575\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.9063\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.8546\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.8019\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.7514\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.7052\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.6619\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.6240\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.5908\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.5626\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.5406\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.5205\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.5005\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.4864\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.4773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "    "
      ],
      "metadata": {
        "id": "8BieBYsUOdHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "VZ9yXF8mOpwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOLFQzPXOwrI",
        "outputId": "86087dfa-cbdb-4fe2-eda1-0187e7fe1a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Thou parted woman in these no all, freedom; I swear\n",
            "Upon his timeless bed-chest in my celse.\n",
            "\n",
            "LORD FITZBEON:\n",
            "Thou canst determine.\n",
            "\n",
            "Second Murderer:\n",
            "What, ho! you must perforce! Can you beholding to you home\n",
            "behind; and break the bidding for a limpter leave,\n",
            "And let my fault concluded by him.\n",
            "I have forgot why I'll bring all these woes!\n",
            "\n",
            "WARWICK:\n",
            "This sparingly three draw on the openings here,\n",
            "And, by He, that hath suffer'd their deaths that you bear\n",
            "Upon exightions and officers, hourisies,\n",
            "Which alive honesty to choose and sullens,\n",
            "but by subjects still a man's wife against his car\n",
            "Abodies his beholding for their villain:\n",
            "My noble earlies, whom here you should do well?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "By Him that rathes stir.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Was never mountain on the gower of these quarts:\n",
            "I will marvel one of those from your brued means love.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "Richmond, let it be drunk, none.\n",
            "Fairly let's trust her for the man-lies; that\n",
            "He was never more ado. indeed! Come,\n",
            "What comfortable hast \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.2006418704986572\n"
          ]
        }
      ]
    }
  ]
}